{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Automatic speech recognition (ASR) converts a speech signal to text.\n",
        "\n",
        "--> Audio as input --> Text as output\n",
        "\n",
        "Examples: Siri and Alexa\n",
        "\n",
        "This demo teaches how to fine-tune to Wav2Vec2 on the MInDS-14 dataset to transcribe audio to text.\n",
        "\n",
        "Use the fine-tuned model for inference.\n",
        "\n",
        "Task page on HF : https://huggingface.co/tasks/automatic-speech-recognition\n",
        "\n"
      ],
      "metadata": {
        "id": "3HcogYxqj5rf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ2xU_ccj3H6"
      },
      "outputs": [],
      "source": [
        "# Install all the libraries\n",
        "!pip install transformers datasets evaluate jiwer huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to HF HUB\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "FK_6Zie-lyon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some info related to dataset\n",
        "\n",
        "MINDS-14 is training and evaluation resource for intent detection task with spoken data.\n",
        "\n",
        "Covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties."
      ],
      "metadata": {
        "id": "Ppnhi_rVnG_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To get rid of of \"NotImplementedError\" in dataset loading\n",
        "!pip install datasets==3.6.0"
      ],
      "metadata": {
        "id": "H56MQx82qFP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MInDS-14 Dataset\n",
        "from datasets import load_dataset, Audio\n",
        "\n",
        "minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train[:100]\")"
      ],
      "metadata": {
        "id": "DeAPHL-kmjI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset's train split into a train and test set with the Dataset.train_test_split method\n",
        "minds = minds.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "VwTT4_y8pcLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the dataset\n",
        "minds"
      ],
      "metadata": {
        "id": "7--MFUTBqtzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"remove_columns\" method docs:\n",
        " https://huggingface.co/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Dataset.remove_columns"
      ],
      "metadata": {
        "id": "J8id-kj6q8rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # This demo focuses on audio and transcription. Remove the other columns with remove_columns methods:\n",
        " minds = minds.remove_columns([\"english_transcription\",\"intent_class\", \"lang_id\"]) # colum names to be removed."
      ],
      "metadata": {
        "id": "jPZBD1FYrK9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look the data again.\n",
        "minds[\"train\"][0]"
      ],
      "metadata": {
        "id": "xJePrevkr9BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## There are two fields in the previous output that we need to understand\n",
        "* audio: a 1-dimensional array of the speech signal that must be called to load and resample the audio file\n",
        "* transcription: the target text."
      ],
      "metadata": {
        "id": "s6Hcomxat6oS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "qVxl5-aiuOJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Wav2Vec2 processor to process the audio signal:\n",
        "\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base\")"
      ],
      "metadata": {
        "id": "Nn-lo50wuQ5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The minds-14 dataset has a sampling rate of 8000Hz, which means you will need to resample the dataset to 16000Hz to use the pretrained Wav2Vec2 model:"
      ],
      "metadata": {
        "id": "f8NnoJEnunBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample the 8000Hz to 16000Hz dataset\n",
        "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "# Check the data again\n",
        "minds[\"train\"][0]"
      ],
      "metadata": {
        "id": "qINXAgk-u0ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Transcription above shows that the text contains a mix of uppercase and lowercase characters. However Wav2Vec2 tokenizer is only trained on uppercase characters so you'll need to make sure the text matches the tokenizer's vocabulary."
      ],
      "metadata": {
        "id": "tYYRFyYsyzRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def uppercase(example):\n",
        "  return {\"transcription\": example[\"transcription\"].upper()}\n",
        "\n",
        "minds = minds.map(uppercase)"
      ],
      "metadata": {
        "id": "daxaoSxZzFvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to create a preprocessing function that:\n",
        "1. Calls the audio column in order to load and resample the audio file.\n",
        "2. Extracts the input_values from the audio file and tokenize the transcription column with the processor"
      ],
      "metadata": {
        "id": "qjXkcTQhzTsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "  audio = batch[\"audio\"]\n",
        "  batch = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], text=batch[\"transcription\"])\n",
        "  batch[\"input_length\"] = len(batch[\"input_values\"][0])\n",
        "  return batch"
      ],
      "metadata": {
        "id": "eCUCJrTZ0fuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To apply the preprocessing function (prepare_dataset) we use the map function from datasets\n",
        "# num_proc is used to speed up the mapping process\n",
        "# We use remove_column because we dont need train part\n",
        "encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names[\"train\"], num_proc=4)"
      ],
      "metadata": {
        "id": "D-kHJOvMxAnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE : Transformers does not have data collator for ASR, so you will need to adapt DataCollatorWithPadding to create a batch of examples.\n",
        "\n",
        "This will also implement dynamic padding text and labels to the length of the longest element in its batch (instead of entire dataset) so they are a uniform length.\n",
        "\n",
        "Unlike other collators, this specific data collator needs to apply a different padding method to input_values and labels:"
      ],
      "metadata": {
        "id": "j1V8Lj5ux2m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "  processor: AutoProcessor\n",
        "  padding: Union[bool, str] = \"longest\"\n",
        "\n",
        "  def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "    # split inputs and labels since they have to be of different lengths and need\n",
        "    # different padding methods\n",
        "    input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n",
        "    label_features = [{\"input_ids\": feature[\"labesl\"]} for feature in features]\n",
        "\n",
        "    batch = self.processor.pad(input_features, padding = self.padding, return_tensors = \"pt\")\n",
        "\n",
        "    labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors=\"pt\")\n",
        "\n",
        "    # replace padding with -100 to ignore loss correctly\n",
        "    labels = labels_batch[\"input_features\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "zvfaM2OdKTfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we need to instantiate your DataCollatorForCTCWithPadding:\n",
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=\"longest\")"
      ],
      "metadata": {
        "id": "RMqMmkKEa1Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate\n",
        "Including a metric during training is often helpful for evaluating your model's performance.\n",
        "\n",
        "You can quickly load an evaluation method wih the ðŸ¤— Evaluate library (https://huggingface.co/docs/evaluate/index).\n",
        "\n",
        "For this task, we used the word error rate (WER) metric;\n",
        "\n",
        " (refer to the ðŸ¤— Evaluate quick tour to learn more about loading and computing metrics): https://huggingface.co/docs/evaluate/a_quick_tour"
      ],
      "metadata": {
        "id": "HZWZf9K2bH8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "wer = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "mih1lEoIs9u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function that passes your predictions and labels to compute to calculate the WER:\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  pred_logits = pred.predictions\n",
        "  pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "  pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "  pred_str = processor.batch_decode(pred_ids)\n",
        "  label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "  wer = wer.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "  return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "4UcZKdQAtYV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: compute_metrics function is ready to go: it will be used in training process."
      ],
      "metadata": {
        "id": "1mU3rAnivhBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train\n",
        "\n",
        "Basic Tutorial on Fine-tuning a model with the PyTorch Trainer: https://huggingface.co/docs/transformers/en/training#train-with-pytorch-trainer"
      ],
      "metadata": {
        "id": "PY3WKZc4wAmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning steps\n",
        "# 1. Load Wav2Vec2 with AutoModelForCTC.\n",
        "# 2. Specificy the reduction to apply with the ctc_loss_reduction parameter.\n",
        "from transformers import AutoModelForCTC, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    ctc_loss_reduction = \"mean\",\n",
        "    pad_token_id = processor.tokenizer.pad_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "DW17nvzGwBwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training hyperparameters\n",
        "# more info: https://huggingface.co/docs/transformers/v4.52.3/en/main_classes/trainer#transformers.TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"my_asr_mind_model\", # output directory\n",
        "    per_device_train_batch_size = 8, # batch size per accelerator core/CPU in training\n",
        "    gradient_accumulation_steps = 2, # Number of updates steps to accumulate the gradients for , before performing a backward/update pass.\n",
        "    learning_rate=1e-5, #The initial learning rate for AdamW optimizer.\n",
        "    warmup_steps= 500, # Number of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio.\n",
        "    max_steps=2000, # If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs. For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until max_steps is reached\n",
        "    gradient_checkpointing=True, #If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
        "    fp16=True, # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
        "    group_by_length=True, # Whether or not to group together samples of roughly the same length in the training dataset (to minimize padding applied and be more efficient). Only useful if applying dynamic padding.\n",
        "    eval_strategy=\"steps\", # The evaluation strategy to adopt during training. Possible values are: \"no\": No evaluation is done during training. \"steps\": Evaluation is done (and logged) every eval_steps. \"epoch\": Evaluation is done at the end of each epoch.\n",
        "    per_device_eval_batch_size=8, # batch size per accelerator core/CPU in evaluation\n",
        "    save_steps = 1000, #  Number of updates steps before two checkpoint saves if save_strategy=\"steps\". Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    eval_steps = 1000, # Number of update steps between two evaluations if eval_strategy=\"steps\". Will default to the same value as logging_steps if not set. Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    logging_steps = 25, #Number of update steps between two logs if logging_strategy=\"steps\". Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    load_best_model_at_end = True, # Whether or not to load the best model found during training at the end of training. When this option is enabled, the best checkpoint will always be saved. See save_total_limit for more.\n",
        "    metric_for_best_model = \"wer\", # Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models. Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\"\n",
        "    greater_is_better = False, # Use in conjunction with load_best_model_at_end and metric_for_best_model to specify if better models should have a greater metric or not. Will default to: True if metric_for_best_model is set to a value that doesnâ€™t end in \"loss\". False if metric_for_best_model is not set, or set to a value that ends in \"loss\".\n",
        "    push_to_hub = True, # Whether or not to push the model to the Hub every time the model is saved. If this is activated, output_dir will begin a git directory synced with the repo (determined by hub_model_id) and the content will be pushed each time a save is triggered (depending on your save_strategy).\n",
        ")"
      ],
      "metadata": {
        "id": "X6sVB4dP0QM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer (\n",
        "    model = model, #the model to train, evaluate or use for predictions. If not provided, a model_init must be passed.\n",
        "    args = training_args, # The arguments to tweak for training. Will default to a basic instance of TrainingArguments with the output_dir set to a directory named tmp_trainer in the current directory if not provided.\n",
        "    train_dataset=encoded_minds[\"train\"], # The dataset to use for training. If it is a Dataset, columns not accepted by the model.forward() method are automatically removed.\n",
        "    eval_dataset = encoded_minds[\"test\"], # The dataset to use for evaluation. If it is a Dataset, columns not accepted by the model.forward() method are automatically removed. If it is a dictionary, it will evaluate on each dataset prepending the dictionary key to the metric name.\n",
        "    processing_class=processor, # Processing class used to process the data. If provided, will be used to automatically process the inputs for the model, and it will be saved along the model to make it easier to rerun an interrupted training or reuse the fine-tuned model. This supersedes the tokenizer argument, which is now deprecated.\n",
        "    data_collator=data_collator, # The function to use to form a batch from a list of elements of train_dataset or eval_dataset. Will default to default_data_collator() if no processing_class is provided, an instance of DataCollatorWithPadding otherwise if the processing_class is a feature extractor or tokenizer.\n",
        "    compute_metrics = compute_metrics, # The function that will be used to compute metrics at evaluation.\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "uFtfATOGp3_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once training is completed, share your model to the Hub with the push_to_hub() method so it can be accessible to everyone:\n",
        "\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "AJwKK6X2tIdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "hhjg6tyMtcsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the fine-tuned model for inference"
      ],
      "metadata": {
        "id": "blSbS3cImOKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a audio file\n",
        "# Run inference on it\n",
        "# Resample the sampling rate of the audio file\n",
        "\n",
        "from datasets import load_dataset, Audio\n",
        "\n",
        "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
        "audio_file = dataset[0][\"audio\"][\"path\"]"
      ],
      "metadata": {
        "id": "oltkK4gOtcRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to try out the fine-tuned mode for inference is to use it in a pipeline()"
      ],
      "metadata": {
        "id": "W7VUetPDm2Zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a pipeline for ASR with your model\n",
        "from transformers import pipeline\n",
        "\n",
        "transcriber = pipeline(\"automatic-speech-recognition\", model=\"CanerCo/my_asr_mind_model\")\n",
        "transcriber(audio_file)"
      ],
      "metadata": {
        "id": "WuO7GHX4m-CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch Inference"
      ],
      "metadata": {
        "id": "D5JBe_sMnZCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a processor to preprocess the audio file\n",
        "# return the input as PyTorch tensors:\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"CanerCo/my_asr_mind_model\")\n",
        "inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
        "\n",
        "# Pass your inputs to the model\n",
        "from transformers import AutoModelForCTC\n",
        "\n",
        "model = AutoModelForCTC.from_pretrained(\"CanerCo/my_asr_mind_model\")\n",
        "with torch.no_grad():\n",
        "  logits = model(**inputs).logits"
      ],
      "metadata": {
        "id": "hWcM4EVgncXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted input_ids with the higest probability, and use the processor to decode the predicted input_ids back into text:\n",
        "import torch\n",
        "\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.batch_decode(predicted_ids)\n",
        "transcription"
      ],
      "metadata": {
        "id": "ytzfOIiWn1_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}