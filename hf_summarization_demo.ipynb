{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Goals\n",
        "* Finetune T5 on a specific dataset for abstractive summarization\n",
        "* Use the model for inference"
      ],
      "metadata": {
        "id": "V47v2tLOBKFq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqwR7HRnBDmm"
      },
      "outputs": [],
      "source": [
        "# install the libraries\n",
        "!pip install transformers datasets evaluate rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "g6bLyzEZBhr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update datasets\n",
        "!pip install -U datasets"
      ],
      "metadata": {
        "id": "MHQ9RbCKDrI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"billsum\", split=\"ca_test\")"
      ],
      "metadata": {
        "id": "So2IJZ7BBpsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into train and test chunks %80 training %20 test\n",
        "data = data.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "wuMPPly7B69p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data) # Training-test distribution"
      ],
      "metadata": {
        "id": "C4o1M9UZENv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of data\n",
        "print(data['train'][0])"
      ],
      "metadata": {
        "id": "JCC4raJVEeFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = the text of the bill as input to the model\n",
        "# summary = a condensed version of text as the target for the model\n",
        "# Preprocessing\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"google-t5/t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
      ],
      "metadata": {
        "id": "pyekTZACErdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing function:\n",
        "# 1-) Prefix the input with a prompt so T5 knows this is a summarization task.\n",
        "prefix = \"summarize: \"\n",
        "# 2-) Use the keyword text_target argument when tokenizing labels\n",
        "def preprocess_function(examples):\n",
        "  inputs = [prefix + doc for doc in examples[\"text\"]]\n",
        "  model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "  # 3-) Truncate sequences to be no longer than the maximum length set by the max_length parameter\n",
        "  labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "  return model_inputs\n"
      ],
      "metadata": {
        "id": "VY61k2ooE5FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the map method to preprocess function over the dataset\n",
        "tokenized_data = data.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "lfnlOUxjIN73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "# Create a batch example using Datacollatorforseq2seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ],
      "metadata": {
        "id": "FGjgZuNOIejR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will include a metric during training (rouge)\n",
        "# so it will be helpful for evaluating the model's performance\n",
        "\n",
        "import evaluate\n",
        "rouge = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "FTHsrmtkI0j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The function that passes your predictions and labels to compute to calculate the ROUGE metric\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "  prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "  return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "NOPh1vncLfaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we are training our model.\n",
        "# First load T5 with AutoModelForSeq2SeqLM\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "BoBKTwdnMmri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the training hyperparameters in Seq2SeqTrainingArguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"my_awesome_summarization_model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=9,\n",
        "    per_device_eval_batch_size=9,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=4,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False, # Change bf16=True for XPU\n",
        "    push_to_hub=True,\n",
        "    report_to = \"none\"\n",
        ")\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Lhgjnq5xM8y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "5COgwZ-oYaxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "JBPP0v4lU7pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use this model for inference now.\n",
        "# First have a text to be summarized. (snow white story)\n",
        "# For summarization you should prefix your input as shown:\n",
        "text = \"summarize: Once upon a time, in a faraway kingdom, there was a kind and beautiful princess named Snow White. She had skin as white as snow, lips as red as roses, and hair as black as coal. But she lived with her stepmother, the Queen, who was beautiful on the outside but jealous and cruel on the inside.\""
      ],
      "metadata": {
        "id": "or8NEyfXU85T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"CanerCoban/my_awesome_summarization_model\")\n",
        "summarizer(text)"
      ],
      "metadata": {
        "id": "wV9flnuJWTrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can also manually replicate the results of the pipeline\n",
        "# Tokenize the text anad return the input_ids as PyTorch tensors:\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"CanerCoban/my_awesome_summarization_model\")\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "YXEKdHs2Y23Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the generate method to create the summarization.\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"CanerCoban/my_awesome_summarization_model\")\n",
        "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)"
      ],
      "metadata": {
        "id": "a4ujJ7zEZdB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the generated token ids back into text:\n",
        "tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "QmywEPT1ZyWa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}